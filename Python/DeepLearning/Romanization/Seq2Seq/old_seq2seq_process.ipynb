{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 14:45:26.321902: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-12 14:45:26.364359: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import ast # To read-in a dict file\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from hangul_utils import split_syllables, join_jamos\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/tyson/Private/codes/romanization/final_dict.pkl\", \"rb\") as f:\n",
    "    result_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_list = list(result_dict.keys())\n",
    "eng_list = list(result_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1434 Daeguoegwaksunhwan Expressway\n"
     ]
    }
   ],
   "source": [
    "for idx, item in enumerate(eng_list):\n",
    "    if ' ' in item:\n",
    "        print(idx, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대구외곽순환고속도로'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor_list[1434]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Daeguoegwaksunhwan Expressway'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor_list.pop(1434)\n",
    "eng_list.pop(1434)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4717 3sandan\n",
      "5358 Je3ttanggul-ro\n",
      "5370 Gukgajeongwon1ho-gil\n"
     ]
    }
   ],
   "source": [
    "for idx, item in enumerate(eng_list):\n",
    "    if re.search(r'[0-9]', item):\n",
    "        print(idx, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [4717, 5358, 5370][::-1]: # Reverse the list as it modifies the indices\n",
    "    del kor_list[i]\n",
    "    del eng_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {}\n",
    "for i in range(len(kor_list)):\n",
    "    new_dict[kor_list[i]] = eng_list[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86191"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_list = list(new_dict.keys())\n",
    "eng_list = list(new_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of characters : 36\n"
     ]
    }
   ],
   "source": [
    "# Get the number of the largest string in the input\n",
    "maxrow_en = 0\n",
    "for inputstring in eng_list: # Korean is always less in size\n",
    "    if len(inputstring) >= maxrow_en:\n",
    "        maxrow_en = len(inputstring) # update\n",
    "print(f\"Maximum number of characters : {maxrow_en}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of characters : 28\n"
     ]
    }
   ],
   "source": [
    "# Get the number of the largest string in the input\n",
    "maxrow_ko = 0\n",
    "for inputstring in kor_list:\n",
    "    inputstring = split_syllables(inputstring)\n",
    "    if len(inputstring) >= maxrow_ko:\n",
    "        maxrow_ko = len(inputstring) # update\n",
    "print(f\"Maximum number of characters : {maxrow_ko}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding to transform the input characters to tensor:\n",
    "def one_hot_encode(string, maxrow, lang = \"en\"):\n",
    "    if lang == \"en\": # USE the range from A->Z 65 -> 90. UPDATE: MAKE THEM ALL UPPERCASE and then use fewer columns. 26 + 1 (hyphen) columns in total.\n",
    "        arr = np.zeros(shape = (maxrow, 27))\n",
    "        string = string[::-1] # Reversing is better to establish 'communication' between encoder and decoder model // https://arxiv.org/pdf/1409.3215.pdf\n",
    "        for idx, char in enumerate(string): # for each character in the string \n",
    "            if not(65 <= ord(char) <= 90) and not (97 <= ord(char) <= 122) and not(ord(char) == 45): # if it's not alphabetical or hyphen\n",
    "                warnings.warn(\"Preprocess your input. The string contains non-alphabetical character.\")\n",
    "                print(f\"The character is {char}\")\n",
    "                raise Exception\n",
    "            else: # If the letter is english\n",
    "                try:\n",
    "                    char = char.upper() # Make it upper case\n",
    "                    arr[idx][ord(char)-65] = 1\n",
    "                except:\n",
    "                    arr[idx][26] = 1 # If it's a hyphen then let column 26 be 1\n",
    "                \n",
    "        return arr\n",
    "    elif lang == \"ko\": # ord of korean starts from 12593 -> 12643 = 12643 - 12593 + 1 = 51 + 2(tab(start), newline(end)) https://github.com/rstudio/keras/blob/main/vignettes/examples/lstm_seq2seq.py\n",
    "        arr = np.zeros(shape = (maxrow, 53))\n",
    "        # Since Korean is the target seq, the one we are trying to predict, use '\\t' as start token and '\\n' as end of sequence token\n",
    "        jamos = split_syllables(string) # Get jamos first\n",
    "        jamos = '\\t' + jamos + '\\n'\n",
    "        for idx, char in enumerate(jamos):\n",
    "            if ord(char) == 9: # if tab(start of sequence)\n",
    "                arr[idx][51] = 1\n",
    "            elif ord(char) == 10: # if newline(end of sequence)\n",
    "                arr[idx][52] = 1    \n",
    "            elif 12593 <= ord(char) <= 12643:\n",
    "                arr[idx][ord(char)-12593] = 1\n",
    "            else:\n",
    "                warnings.warn(\"Preprocess your input. The string contains non-korean character.\")\n",
    "                print(f\"The character is {char}\")\n",
    "                raise Exception\n",
    "                \n",
    "        return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed length:\n",
    "def one_hot_tensor_matrix(inputdata, lang = 'en'):\n",
    "    # Languages:\n",
    "    if lang == \"en\":\n",
    "        ncol = 27 # Depends on what function you use. \n",
    "        maxrow = maxrow_en\n",
    "    elif lang == \"ko\":\n",
    "        ncol = 53\n",
    "        maxrow = maxrow_ko + 2 # Since we added '\\t' and '\\n'\n",
    "    else:\n",
    "        warnings.warn(\"Check your language please. There are only Korean and English.\")\n",
    "        raise Exception\n",
    "    initial_mat = np.zeros(shape=(len(inputdata), maxrow, ncol)) # The shape is of (batch size, nrow, ncol)\n",
    "    for idx, string in enumerate(inputdata):\n",
    "        temparr = one_hot_encode(string, maxrow, lang)\n",
    "        initial_mat[idx] = temparr # Update initial matrix\n",
    "    \n",
    "    # Change the shape of the matrix:\n",
    "    final_mat = np.reshape(initial_mat, newshape = (len(inputdata), maxrow, ncol))\n",
    "    return tf.convert_to_tensor(final_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_decode(mat, lang = 'en'):\n",
    "    char_list = [] # To store the characters, later to be used in joining\n",
    "    if lang == 'en': # if the language is english\n",
    "        for row in mat:\n",
    "            if np.max(row) == 0: # If the row is an empty one\n",
    "                continue\n",
    "            else: # If our row is not an empty row\n",
    "                code = np.argmax(row) # Since 1 is the largest number in our one hot encoded, we use np.argmax()\n",
    "                if code == 26: # If it's a hyphen\n",
    "                    char_list.append(chr(45))\n",
    "                else:\n",
    "                    char_list.append(chr(code+65))\n",
    "        string = \"\".join(char_list)\n",
    "    elif lang == 'ko':\n",
    "        for row in mat:\n",
    "            if np.max(row) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                code = np.argmax(row)\n",
    "                if code == 52:\n",
    "                    char_list.append(chr(9))\n",
    "                elif code == 53:\n",
    "                    char_list.append(chr(10))\n",
    "                else:\n",
    "                    char_list.append(chr(code+12593))\n",
    "        string = join_jamos(\"\".join(char_list))\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum length of the input words in each kor_list and eng_list. This will be used when we initialize the model. Use the above functions accordingly. For fixed: one_hot_tensor_matrix\n",
    "decoder_input = one_hot_tensor_matrix(kor_list, lang = 'ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first line since decoder target preceeds the decoder input by one timestep so, a 'start' signal would give an output of a word to be compared with decoder target\n",
    "decoder_target = decoder_input[:, 1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(86191, 1, 53), dtype=float64, numpy=\n",
       "array([[[0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.]]])>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a new line and add it to the second dimension of the decoder target tensor so that we have equal shape with the predicted val and the actual val\n",
    "# Fixed length\n",
    "new_line = tf.zeros((decoder_target.shape[0], 1, decoder_target.shape[2]), tf.float64)\n",
    "new_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Length\n",
    "decoder_target = tf.concat([decoder_target, new_line], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(decoder_target.shape == decoder_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = one_hot_tensor_matrix(eng_list, lang = 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder_input) == len(decoder_target) == len(decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = tf.cast(encoder_input, tf.float32)\n",
    "decoder_input = tf.cast(decoder_input, tf.float32)\n",
    "decoder_target = tf.cast(decoder_target, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"LSTM_encoder_in.pkl\", \"wb\") as f:\n",
    "    pickle.dump(encoder_input, f)\n",
    "with open(\"LSTM_decoder_in.pkl\", \"wb\") as f:\n",
    "    pickle.dump(decoder_input, f)\n",
    "with open(\"LSTM_decoder_targ.pkl\", \"wb\") as f:\n",
    "    pickle.dump(decoder_target, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 06:56:58) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "266608b04208d70543cb49c8748fa82829af85a97fbaee1332ca8e44492d567f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
