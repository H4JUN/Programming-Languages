{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94efb496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 16:33:52.717472: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-12 16:33:52.761826: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from hangul_utils import split_syllables, join_jamos\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Masking\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2d3ceb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 16:33:53.456337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 16:33:53.475438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 16:33:53.475577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "# Without this, we might get some unexpected errors; still not working\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e85c5dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding code:\n",
    "def one_hot_decode(mat, lang = 'en'):\n",
    "    char_list = [] # To store the characters, later to be used in joining\n",
    "    if lang == 'en': # if the language is english\n",
    "        for row in mat:\n",
    "            if np.max(row) == 0: # If the row is an empty one\n",
    "                continue\n",
    "            else: # If our row is not an empty row\n",
    "                code = np.argmax(row) # Since 1 is the largest number in our one hot encoded, we use np.argmax()\n",
    "                if code == 26: # If it's a hyphen\n",
    "                    char_list.append(chr(45))\n",
    "                else:\n",
    "                    char_list.append(chr(code+65))\n",
    "        string = \"\".join(char_list)\n",
    "    elif lang == 'ko':\n",
    "        for row in mat:\n",
    "            if np.max(row) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                code = np.argmax(row)\n",
    "                if code == 51:\n",
    "                    char_list.append(chr(9))\n",
    "                elif code == 52:\n",
    "                    char_list.append(chr(10))\n",
    "                else:\n",
    "                    char_list.append(chr(code+12593))\n",
    "        string = join_jamos(\"\".join(char_list))\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50e88ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_matrix(inputTensor, lang = 'en'):\n",
    "    str_list = []\n",
    "    inputTensor = np.reshape(inputTensor, (inputTensor.shape[0], inputTensor.shape[1], inputTensor.shape[2]))\n",
    "    for tensor in inputTensor: # for each matrix\n",
    "        string = one_hot_decode(tensor, lang)\n",
    "        str_list.append(string)\n",
    "    return str_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abffad4f",
   "metadata": {
    "id": "031DLazPmmmO"
   },
   "source": [
    "# Managing Input and Output: `PreProcessing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe187f3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T14:29:11.390176Z",
     "iopub.status.busy": "2022-02-12T14:29:11.389493Z",
     "iopub.status.idle": "2022-02-12T14:29:11.748042Z",
     "shell.execute_reply": "2022-02-12T14:29:11.747322Z",
     "shell.execute_reply.started": "2022-02-12T14:29:11.390139Z"
    },
    "id": "ea7d46a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 16:33:53.625251: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-12 16:33:53.627460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 16:33:53.627613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 16:33:53.627671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 16:33:53.950624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 16:33:53.950740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 16:33:53.950794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 16:33:53.950850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13499 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# read in data:\n",
    "with open(\"LSTM_encoder_in.pkl\", 'rb') as f:\n",
    "    encoder_in = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be0225b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T14:29:12.642955Z",
     "iopub.status.busy": "2022-02-12T14:29:12.642420Z",
     "iopub.status.idle": "2022-02-12T14:29:13.064803Z",
     "shell.execute_reply": "2022-02-12T14:29:13.064068Z",
     "shell.execute_reply.started": "2022-02-12T14:29:12.642916Z"
    },
    "id": "w81z6W4l2pqn"
   },
   "outputs": [],
   "source": [
    "with open(\"LSTM_decoder_in.pkl\", 'rb') as f:\n",
    "    decoder_in = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddf42d4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T14:29:13.728019Z",
     "iopub.status.busy": "2022-02-12T14:29:13.727553Z",
     "iopub.status.idle": "2022-02-12T14:29:24.397449Z",
     "shell.execute_reply": "2022-02-12T14:29:24.396709Z",
     "shell.execute_reply.started": "2022-02-12T14:29:13.727985Z"
    },
    "id": "ceb2827e"
   },
   "outputs": [],
   "source": [
    "with open(\"LSTM_decoder_targ.pkl\", 'rb') as f:\n",
    "    decoder_targ = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f157b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([86191, 36, 27])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_in.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c05928b",
   "metadata": {},
   "source": [
    "### The inputs letters are reversed according to the paper that suggested reversing the order allows LSTM to \"establish connection\".\n",
    "### https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf See Page 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eadaf8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ODHMANALLOEJ'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_decode(encoder_in[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b251d359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JEOLLANAMHDO : \t전라남도\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{one_hot_decode(encoder_in[0], lang='en')[::-1]} : {one_hot_decode(decoder_in[0], lang='ko')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8f27b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAEGUNHGIL : \t백운길\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{one_hot_decode(encoder_in[10], lang='en')[::-1]} : {one_hot_decode(decoder_in[10], lang='ko')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0619a1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T14:30:25.967814Z",
     "iopub.status.busy": "2022-02-12T14:30:25.967227Z",
     "iopub.status.idle": "2022-02-12T14:30:25.973871Z",
     "shell.execute_reply": "2022-02-12T14:30:25.972967Z",
     "shell.execute_reply.started": "2022-02-12T14:30:25.967768Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder_in) == len(decoder_in) == len(decoder_targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f1aa76",
   "metadata": {
    "id": "AMbLVcnOmxE5"
   },
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eac2e349",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T14:30:29.015660Z",
     "iopub.status.busy": "2022-02-12T14:30:29.015116Z",
     "iopub.status.idle": "2022-02-12T14:30:29.904066Z",
     "shell.execute_reply": "2022-02-12T14:30:29.903324Z",
     "shell.execute_reply.started": "2022-02-12T14:30:29.015622Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n",
    "# https://github.com/rstudio/keras/blob/main/vignettes/examples/lstm_seq2seq.py\n",
    "\n",
    "latent_dim = 128\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "# encoder_inputs = Input(shape=(36,27))\n",
    "encoder_inputs = Input(shape=(None, 27))\n",
    "mask = Masking(mask_value=0.0)\n",
    "masked_inputs = mask(encoder_inputs)\n",
    "encoder1_1 = LSTM(latent_dim, return_sequences = True, return_state=True, name = \"enc_1\")\n",
    "encoder1_2 = LSTM(latent_dim, return_state = True, name = \"enc_2\")\n",
    "encoder_outputs1_1, state_h1, state_c1 = encoder1_1(masked_inputs)\n",
    "encoder_outputs, state_h2, state_c2 = encoder1_2(encoder_outputs1_1)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h1, state_c1, state_h2, state_c2]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, 53))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm1_1 = LSTM(latent_dim, return_sequences=True, return_state=True, name = \"dec_1\")\n",
    "decoder_lstm1_2 = LSTM(latent_dim, return_sequences=True, return_state=True, name = \"dec_2\")\n",
    "decoder_outputs1_1, _, _ = decoder_lstm1_1(decoder_inputs, initial_state=encoder_states[-2:]) # Here I'm using a -> b = Latent Space = b -> a. Unpack/Decode using the 2nd layer's initial states and then the 1st.\n",
    "decoder_outputs, _, _ = decoder_lstm1_2(decoder_outputs1_1, initial_state = encoder_states[:2])\n",
    "decoder_dense = Dense(53, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf5eb56d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T14:30:30.232909Z",
     "iopub.status.busy": "2022-02-12T14:30:30.232145Z",
     "iopub.status.idle": "2022-02-12T14:30:30.244094Z",
     "shell.execute_reply": "2022-02-12T14:30:30.243308Z",
     "shell.execute_reply.started": "2022-02-12T14:30:30.232862Z"
    },
    "id": "9wDa6p8MAvsK",
    "outputId": "acbbd05f-5dad-457e-9234-c2c85665cc25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None, 27)]   0           []                               \n",
      "                                                                                                  \n",
      " masking (Masking)              (None, None, 27)     0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " enc_1 (LSTM)                   [(None, None, 128),  79872       ['masking[0][0]']                \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None, 53)]   0           []                               \n",
      "                                                                                                  \n",
      " enc_2 (LSTM)                   [(None, 128),        131584      ['enc_1[0][0]']                  \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " dec_1 (LSTM)                   [(None, None, 128),  93184       ['input_2[0][0]',                \n",
      "                                 (None, 128),                     'enc_2[0][1]',                  \n",
      "                                 (None, 128)]                     'enc_2[0][2]']                  \n",
      "                                                                                                  \n",
      " dec_2 (LSTM)                   [(None, None, 128),  131584      ['dec_1[0][0]',                  \n",
      "                                 (None, 128),                     'enc_1[0][1]',                  \n",
      "                                 (None, 128)]                     'enc_1[0][2]']                  \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 53)     6837        ['dec_2[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 443,061\n",
      "Trainable params: 443,061\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6ad856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "epochs = 50\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77cfa0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e59fd3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71038f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 16:34:01.684017: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401\n",
      "2022-12-12 16:34:01.758836: W tensorflow/core/common_runtime/forward_type_inference.cc:332] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\twhile inferring type of node 'cond_40/output/_23'\n",
      "2022-12-12 16:34:01.901550: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2155/2155 [==============================] - 36s 15ms/step - loss: 0.6628 - accuracy: 0.5514 - val_loss: 0.3790 - val_accuracy: 0.9214\n",
      "Epoch 2/50\n",
      "2155/2155 [==============================] - 29s 14ms/step - loss: 0.2356 - accuracy: 0.9595 - val_loss: 0.2213 - val_accuracy: 0.9636\n",
      "Epoch 3/50\n",
      "2155/2155 [==============================] - 30s 14ms/step - loss: 0.1333 - accuracy: 0.8733 - val_loss: 0.1359 - val_accuracy: 0.2886\n",
      "Epoch 4/50\n",
      "2155/2155 [==============================] - 30s 14ms/step - loss: 0.0948 - accuracy: 0.4643 - val_loss: 0.1158 - val_accuracy: 0.2917\n",
      "Epoch 5/50\n",
      "2155/2155 [==============================] - 30s 14ms/step - loss: 0.0780 - accuracy: 0.4255 - val_loss: 0.1067 - val_accuracy: 0.2974\n",
      "Epoch 6/50\n",
      "2155/2155 [==============================] - 29s 14ms/step - loss: 0.0663 - accuracy: 0.4167 - val_loss: 0.0879 - val_accuracy: 0.2953\n",
      "Epoch 7/50\n",
      "2155/2155 [==============================] - 28s 13ms/step - loss: 0.0570 - accuracy: 0.3759 - val_loss: 0.0786 - val_accuracy: 0.2966\n",
      "Epoch 8/50\n",
      "2155/2155 [==============================] - 28s 13ms/step - loss: 0.0542 - accuracy: 0.3779 - val_loss: 0.0743 - val_accuracy: 0.2975\n",
      "Epoch 9/50\n",
      "2155/2155 [==============================] - 28s 13ms/step - loss: 0.0522 - accuracy: 0.3761 - val_loss: 0.0674 - val_accuracy: 0.2983\n",
      "Epoch 10/50\n",
      "2155/2155 [==============================] - 28s 13ms/step - loss: 0.0477 - accuracy: 0.3765 - val_loss: 0.0645 - val_accuracy: 0.2987\n",
      "Epoch 11/50\n",
      "2155/2155 [==============================] - 28s 13ms/step - loss: 0.0446 - accuracy: 0.3769 - val_loss: 0.0629 - val_accuracy: 0.2991\n",
      "Epoch 12/50\n",
      "2155/2155 [==============================] - 26s 12ms/step - loss: 0.0421 - accuracy: 0.3772 - val_loss: 0.0619 - val_accuracy: 0.2995\n",
      "Epoch 13/50\n",
      "2155/2155 [==============================] - 28s 13ms/step - loss: 0.0407 - accuracy: 0.3774 - val_loss: 0.0627 - val_accuracy: 0.2993\n",
      "Epoch 14/50\n",
      "2155/2155 [==============================] - 30s 14ms/step - loss: 0.0388 - accuracy: 0.3776 - val_loss: 0.0582 - val_accuracy: 0.3000\n",
      "Epoch 15/50\n",
      "2155/2155 [==============================] - 28s 13ms/step - loss: 0.0370 - accuracy: 0.3777 - val_loss: 0.0580 - val_accuracy: 0.2997\n",
      "Epoch 16/50\n",
      "2155/2155 [==============================] - 28s 13ms/step - loss: 0.0346 - accuracy: 0.3779 - val_loss: 0.0530 - val_accuracy: 0.3002\n",
      "Epoch 17/50\n",
      " 947/2155 [============>.................] - ETA: 13s - loss: 0.0322 - accuracy: 0.3785"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrmsprop\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m st_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m----> 4\u001b[0m model\u001b[39m.\u001b[39;49mfit([encoder_in, decoder_in], decoder_targ, batch_size\u001b[39m=\u001b[39;49mbatch_size, epochs\u001b[39m=\u001b[39;49mepochs, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[tensorboard_callback])\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTime taken in seconds: \u001b[39m\u001b[39m%g\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m(time\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39mst_time))\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "st_time = time.time()\n",
    "model.fit([encoder_in, decoder_in], decoder_targ, batch_size=batch_size, epochs=epochs, validation_split=0.2, callbacks=[tensorboard_callback])\n",
    "print(\"Time taken in seconds: %g\" %(time.time()-st_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fac7a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder:\n",
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44c2722b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-29T13:27:43.067391Z",
     "iopub.status.busy": "2021-12-29T13:27:43.067044Z",
     "iopub.status.idle": "2021-12-29T13:27:43.587349Z",
     "shell.execute_reply": "2021-12-29T13:27:43.586398Z",
     "shell.execute_reply.started": "2021-12-29T13:27:43.067347Z"
    },
    "id": "au5qhRKEyYF_"
   },
   "outputs": [],
   "source": [
    "# Decoder:\n",
    "decoder_state_input_h1 = Input(shape=(latent_dim,)) # For the first LSTM layer of enc\n",
    "decoder_state_input_c1 = Input(shape=(latent_dim,))\n",
    "decoder_state_input_h2 = Input(shape=(latent_dim,)) # For the second LSTM layer of enc\n",
    "decoder_state_input_c2 = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h1, decoder_state_input_c1, decoder_state_input_h2, decoder_state_input_c2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs1_1, d_state_h1, d_state_c1 = decoder_lstm1_1(decoder_inputs, initial_state=decoder_states_inputs[-2:], training = False) # h2 c2\n",
    "decoder_outputs, d_state_h2, d_state_c2 = decoder_lstm1_2(decoder_outputs1_1, initial_state = decoder_states_inputs[:2], training = False) # h1 c1\n",
    "decoder_states = [d_state_h1, d_state_c1, d_state_h2, d_state_c2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs = decoder_dense(decoder_outputs, training=False)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36205b46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-29T13:27:43.589862Z",
     "iopub.status.busy": "2021-12-29T13:27:43.589622Z",
     "iopub.status.idle": "2021-12-29T13:27:43.601334Z",
     "shell.execute_reply": "2021-12-29T13:27:43.600045Z",
     "shell.execute_reply.started": "2021-12-29T13:27:43.589821Z"
    },
    "id": "RffTv2j0A2Et",
    "outputId": "d930a875-e4dc-42b0-abc8-5154054e8289"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, 27)]        0         \n",
      "                                                                 \n",
      " masking (Masking)           (None, None, 27)          0         \n",
      "                                                                 \n",
      " enc_1 (LSTM)                [(None, None, 128),       79872     \n",
      "                              (None, 128),                       \n",
      "                              (None, 128)]                       \n",
      "                                                                 \n",
      " enc_2 (LSTM)                [(None, 128),             131584    \n",
      "                              (None, 128),                       \n",
      "                              (None, 128)]                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 211,456\n",
      "Trainable params: 211,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c9823f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-29T13:27:43.605168Z",
     "iopub.status.busy": "2021-12-29T13:27:43.603094Z",
     "iopub.status.idle": "2021-12-29T13:27:43.622539Z",
     "shell.execute_reply": "2021-12-29T13:27:43.621664Z",
     "shell.execute_reply.started": "2021-12-29T13:27:43.605121Z"
    },
    "id": "ctseSImDA4Qj",
    "outputId": "69e1ecdd-4a02-4d17-ae76-717b9c6fcead"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, None, 53)]   0           []                               \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " dec_1 (LSTM)                   [(None, None, 128),  93184       ['input_2[0][0]',                \n",
      "                                 (None, 128),                     'input_5[0][0]',                \n",
      "                                 (None, 128)]                     'input_6[0][0]']                \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " dec_2 (LSTM)                   [(None, None, 128),  131584      ['dec_1[1][0]',                  \n",
      "                                 (None, 128),                     'input_3[0][0]',                \n",
      "                                 (None, 128)]                     'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 53)     6837        ['dec_2[1][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 231,605\n",
      "Trainable params: 231,605\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe1623d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-29T13:27:43.624256Z",
     "iopub.status.busy": "2021-12-29T13:27:43.623868Z",
     "iopub.status.idle": "2021-12-29T13:27:43.648453Z",
     "shell.execute_reply": "2021-12-29T13:27:43.647609Z",
     "shell.execute_reply.started": "2021-12-29T13:27:43.624209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "encoder_model.save(\"./models/char-rnn-encoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e16be3bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-29T13:27:43.652047Z",
     "iopub.status.busy": "2021-12-29T13:27:43.651825Z",
     "iopub.status.idle": "2021-12-29T13:27:43.679393Z",
     "shell.execute_reply": "2021-12-29T13:27:43.678429Z",
     "shell.execute_reply.started": "2021-12-29T13:27:43.652018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "decoder_model.save(\"./models/char-rnn-decoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e99623a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-29T13:39:43.585526Z",
     "iopub.status.busy": "2021-12-29T13:39:43.584685Z",
     "iopub.status.idle": "2021-12-29T13:39:43.601311Z",
     "shell.execute_reply": "2021-12-29T13:39:43.600055Z",
     "shell.execute_reply.started": "2021-12-29T13:39:43.585463Z"
    },
    "id": "WMkMmVbi387U"
   },
   "outputs": [],
   "source": [
    "# One hot encoding to transform the input characters to tensor:\n",
    "def one_hot_encode(string, lang = \"en\"):\n",
    "    if lang == \"en\": # USE the range from A->Z 65 -> 90. UPDATE: MAKE THEM ALL UPPERCASE and then use fewer columns. 26 + 1 (hyphen) columns in total.\n",
    "        arr = np.zeros(shape = (len(string)+1, 27)) # ?? why?\n",
    "        string = string[::-1] # Reversing is better to establish 'communication' between encoder and decoder model // https://arxiv.org/pdf/1409.3215.pdf\n",
    "        for idx, char in enumerate(string): # for each character in the string \n",
    "            if not(65 <= ord(char) <= 90) and not (97 <= ord(char) <= 122) and not(ord(char) == 45): # if it's not alphabetical or hyphen\n",
    "                warnings.warn(\"Preprocess your input. The string contains non-alphabetical character.\")\n",
    "                print(f\"The character is {char}\")\n",
    "                raise Exception\n",
    "            else: # If the letter is english\n",
    "                try:\n",
    "                    char = char.upper() # Make it upper case\n",
    "                    arr[idx][ord(char)-65] = 1\n",
    "                except:\n",
    "                    arr[idx][26] = 1 # If it's a hyphen then let column 26 be 1\n",
    "                \n",
    "        return np.expand_dims(arr, axis=0)\n",
    "    elif lang == \"ko\": # ord of korean starts from 12593 -> 12643 = 12643 - 12593 + 1 = 51 + 2(tab(start), newline(end)) https://github.com/rstudio/keras/blob/main/vignettes/examples/lstm_seq2seq.py\n",
    "        # Since Korean is the target seq, the one we are trying to predict, use '\\t' as start and '\\n' as end of sequence\n",
    "        jamos = split_syllables(string) # Get jamos first\n",
    "        jamos = '\\t' + jamos + '\\n'\n",
    "        arr = np.zeros(shape = (len(jamos)+1, 53))\n",
    "        for idx, char in enumerate(jamos):\n",
    "            if ord(char) == 9: # if tab(start of sequence)\n",
    "                arr[idx][51] = 1\n",
    "            elif ord(char) == 10: # if newline(end of sequence)\n",
    "                arr[idx][52] = 1    \n",
    "            elif 12593 <= ord(char) <= 12643:\n",
    "                arr[idx][ord(char)-12593] = 1\n",
    "            else:\n",
    "                warnings.warn(\"Preprocess your input. The string contains non-korean character.\")\n",
    "                print(f\"The character is {char}\")\n",
    "                raise Exception\n",
    "                \n",
    "        return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aeb6c5a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-29T13:39:43.763161Z",
     "iopub.status.busy": "2021-12-29T13:39:43.762366Z",
     "iopub.status.idle": "2021-12-29T13:39:43.771604Z",
     "shell.execute_reply": "2021-12-29T13:39:43.770558Z",
     "shell.execute_reply.started": "2021-12-29T13:39:43.763124Z"
    },
    "id": "GJAaTzto385A"
   },
   "outputs": [],
   "source": [
    "# Masking takes place so no need to do this \n",
    "def one_hot_tensor_matrix(inputdata, lang = 'en'):\n",
    "    # Languages:\n",
    "    if lang == \"en\":\n",
    "        ncol = 27 # Depends on what function you use. \n",
    "        maxrow = 36\n",
    "    elif lang == \"ko\":\n",
    "        ncol = 53\n",
    "        maxrow = 28 + 2 # Since we added '\\t' and '\\n'\n",
    "    else:\n",
    "        warnings.warn(\"Check your language please. There are only Korean and English.\")\n",
    "        raise Exception\n",
    "    initial_mat = np.zeros(shape=(len(inputdata), maxrow, ncol)) # The shape is of (batch size, nrow, ncol)\n",
    "    for idx, string in enumerate(inputdata):\n",
    "        temparr = one_hot_encode(string, maxrow, lang)\n",
    "        initial_mat[idx] = temparr # Update initial matrix\n",
    "    \n",
    "    # Change the shape of the matrix:\n",
    "    final_mat = np.reshape(initial_mat, newshape = (len(inputdata), maxrow, ncol))\n",
    "    return tf.convert_to_tensor(final_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9db2c4e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-29T13:39:43.960452Z",
     "iopub.status.busy": "2021-12-29T13:39:43.957755Z",
     "iopub.status.idle": "2021-12-29T13:39:43.970614Z",
     "shell.execute_reply": "2021-12-29T13:39:43.969458Z",
     "shell.execute_reply.started": "2021-12-29T13:39:43.960416Z"
    },
    "id": "9rB1QPbB4MQd"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, 53))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, 51] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h1, c1, h2, c2 = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_char = one_hot_decode(output_tokens, lang = \"ko\")\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > 30): # 30 is the length of the longest string in korean in training dataset\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, 53))\n",
    "        target_seq[0, 0, np.argmax(output_tokens)] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h2, c2, h1, c1] # It's confusing here. We reverse the input states. That's because initially the initial states for the decoders were reversed [:2] [-2:].\n",
    "        # Layer 1 of LSTM_decod takes h2 c2 from encoder 2, and Layer 2 of LSTM_decod takes h1 and c1. After training, in the inference setup, the decoder model takes input from h1,c1,h2,c2\n",
    "        # and then reverses the order. After the initial states, in this code, we know that h1 and c1 should be inserted back to LSTM_decod layer 1 because that's where it came from. It needs to use\n",
    "        # its own previous hidden state so that it knows how to move on. By setting the initial state from h(t-1) and c(t-1) the model is able to predict correct just as it was learned in training mode.\n",
    "        \n",
    "        ## The weights are the same but the values of the hidden states and cells change Wh Wc remain the same the h(t-1) and c(t-1) change which helps the decoder know how to move on.\n",
    "    return join_jamos(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8400c7ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-29T13:39:44.622739Z",
     "iopub.status.busy": "2021-12-29T13:39:44.6224Z",
     "iopub.status.idle": "2021-12-29T13:39:44.627066Z",
     "shell.execute_reply": "2021-12-29T13:39:44.625939Z",
     "shell.execute_reply.started": "2021-12-29T13:39:44.622707Z"
    },
    "id": "0Stc2H5l4BZZ"
   },
   "outputs": [],
   "source": [
    "testing = [\"naneun\", \"annyeong\", \"ihwa\", \"seochodong\", \"teheran\", \"itaewon\", \"garosugil\", \"gangnamgu\", \"dongdaemun\", \"hajun\", \"babo\", \"galwolilgil\", \"ahahhahahahahahahhahdsdhahahahaa\"]\n",
    "# tes = one_hot_tensor_matrix(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb6c087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "897b48ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7, 27)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encode(\"naneun\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5abf74c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 808ms/step\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "난은\n",
      "\n",
      "1/1 [==============================] - 1s 795ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "안녕\n",
      "\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "이화\n",
      "\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "서초동\n",
      "\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "테헤란\n",
      "\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "이태원\n",
      "\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "가로수길\n",
      "\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "강남구\n",
      "\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "동대문\n",
      "\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "하준\n",
      "\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "바보\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "갈월일길\n",
      "\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "아하하하하사하사하아리하다여\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in testing:\n",
    "    print(decode_sequence(one_hot_encode(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6923a3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "니가\n",
      "\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "글운개\n",
      "\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "안히면\n",
      "\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "ㅁㅇㅇ여\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = \"niga geurungae a-ni-myeon mwoya\"\n",
    "test = test.split()\n",
    "for token in test:\n",
    "    print(decode_sequence(one_hot_encode(token)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51057baa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-29T13:40:03.574773Z",
     "iopub.status.busy": "2021-12-29T13:40:03.574225Z",
     "iopub.status.idle": "2021-12-29T13:40:03.580435Z",
     "shell.execute_reply": "2021-12-29T13:40:03.579471Z",
     "shell.execute_reply.started": "2021-12-29T13:40:03.574727Z"
    },
    "id": "beGpS-LwfjIg",
    "outputId": "f5e2bce8-5f0b-4722-ec4a-a52ffba0cf21"
   },
   "outputs": [],
   "source": [
    "test = one_hot_encode(\"ihajun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf48956c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-29T13:40:03.922552Z",
     "iopub.status.busy": "2021-12-29T13:40:03.922237Z",
     "iopub.status.idle": "2021-12-29T13:40:04.429423Z",
     "shell.execute_reply": "2021-12-29T13:40:04.427265Z",
     "shell.execute_reply.started": "2021-12-29T13:40:03.922517Z"
    },
    "id": "aPgF22zLgAQm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "이하준\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(decode_sequence(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 06:56:58) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "266608b04208d70543cb49c8748fa82829af85a97fbaee1332ca8e44492d567f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
